{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b217fc-3d9c-4fca-9a22-cdc0928d923a",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17b8789-346c-436d-9965-de4ebfb45bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u0145642\\AppData\\Roaming\\Python\\Python311\\site-packages\\bayesflow\\trainers.py:27: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import bayesflow as bf\n",
    "import os\n",
    "from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numba as nb\n",
    "import copy\n",
    "import random\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import tensorflow as tf\n",
    "import ipynbname\n",
    "RNG = np.random.default_rng(2023)\n",
    "from diffusion_functions import diffusion_trial, diffusion_prior, generate_condition_matrix, diffusion_configurator\n",
    "# Suppress scientific notation for floats\n",
    "np.set_printoptions(suppress=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceed949",
   "metadata": {},
   "source": [
    "## Get file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8fa466",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path = Path(ipynbname.path()).resolve().parent\n",
    "root_dir = notebook_path.parent\n",
    "#print(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59a739-cdfd-438a-8b58-17c0d13b514a",
   "metadata": {},
   "source": [
    "## Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "303b2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path =  root_dir/ \"networks/standard\"\n",
    "checkpoint_path_t_df1 = root_dir/ \"networks/t_df1\"\n",
    "checkpoint_path_t_df3 = root_dir/ \"networks/t_df3\"\n",
    "checkpoint_path_t_df5 = root_dir/ \"networks/t_df5\"\n",
    "checkpoint_path_uniform = root_dir/ \"networks/uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0595a9d",
   "metadata": {},
   "source": [
    "## Set up prior and condition matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b7ffc10-4c7a-4979-8eee-9b8ed56c618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_NAMES = [\n",
    "    \"Drift rate 1\",\n",
    "    \"Drift rate 2\",\n",
    "    \"Boundary separation\",\n",
    "    \"Response bias\",\n",
    "    \"Non-decision time\"\n",
    "]\n",
    "\n",
    "prior = bf.simulation.Prior(prior_fun=diffusion_prior, param_names=PARAM_NAMES)\n",
    "\n",
    "MIN_OBS = 100\n",
    "MAX_OBS = 1000\n",
    "NUM_CONDITIONS=2\n",
    "\n",
    "\n",
    "def random_num_obs(min_obs=MIN_OBS, max_obs=MAX_OBS):\n",
    "   \"\"\"Draws a random number of observations for all simulations in a batch.\"\"\"\n",
    "\n",
    "   return RNG.integers(low=min_obs, high=max_obs + 1)\n",
    "\n",
    "\n",
    "context_gen = bf.simulation.ContextGenerator(\n",
    "    non_batchable_context_fun=random_num_obs,\n",
    "    batchable_context_fun=generate_condition_matrix,\n",
    "    use_non_batchable_for_batchable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3445dc-7169-4ade-92a6-4ce52b6486e2",
   "metadata": {},
   "source": [
    "## Set up and train the DDM estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8d4cc",
   "metadata": {},
   "source": [
    "### Standard estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ae0d80-0738-4e3e-a0cb-763638abafde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Performing 2 pilot runs with the anonymous model...\n",
      "INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 5)\n",
      "INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 592, 2)\n",
      "INFO:root:No optional prior non-batchable context provided.\n",
      "INFO:root:No optional prior batchable context provided.\n",
      "INFO:root:Shape of simulation non-batchable context: ()\n",
      "INFO:root:Could not determine shape of simulation batchable context. Type appears to be non-array: <class 'list'>,                                    so make sure your input configurator takes cares of that!\n",
      "INFO:root:Loaded loss history from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\standard\\history_100.pkl.\n",
      "INFO:root:Networks loaded from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\standard\\ckpt-100\n",
      "INFO:root:Performing a consistency check with provided components...\n",
      "INFO:root:Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amortized_posterior\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ddm_inference (InvertibleN  multiple                  434037    \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " set_transformer (SetTransf  multiple                  46544     \n",
      " ormer)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 480581 (1.83 MB)\n",
      "Trainable params: 480521 (1.83 MB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def diffusion_experiment(theta, design_matrix, num_obs, rng=None, *args):\n",
    "    \n",
    "    out = np.zeros((num_obs, 2))\n",
    "    for n in range(num_obs):\n",
    "        index = design_matrix[n]\n",
    "        rt,resp = diffusion_trial(theta[index], theta[-3], theta[-2], theta[-1])\n",
    "        out[n, :] = np.array([rt,resp])\n",
    "        \n",
    "    out[:,0] = np.log(out[:,0])\n",
    "    \n",
    "    return out\n",
    "\n",
    "simulator = bf.simulation.Simulator(simulator_fun=diffusion_experiment, context_generator=context_gen)\n",
    "\n",
    "model = bf.simulation.GenerativeModel(prior=prior, simulator=simulator)\n",
    "\n",
    "summary_net = bf.networks.SetTransformer(input_dim=4, summary_dim=12)\n",
    "\n",
    "inference_net = bf.networks.InvertibleNetwork(\n",
    "    num_params=len(prior.param_names),\n",
    "    coupling_design=\"interleaved\",\n",
    "    name=\"ddm_inference\",\n",
    ")\n",
    "\n",
    "amortizer = bf.amortizers.AmortizedPosterior(inference_net, summary_net)\n",
    "\n",
    "trainer = bf.trainers.Trainer(\n",
    "    generative_model=model, amortizer=amortizer, configurator=diffusion_configurator\n",
    "    ,checkpoint_path = checkpoint_path\n",
    ")\n",
    "amortizer.summary()\n",
    "\n",
    "#history = trainer.train_online(epochs=100, iterations_per_epoch=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d862a",
   "metadata": {},
   "source": [
    "### Robust estimator (t, df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1070e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Performing 2 pilot runs with the anonymous model...\n",
      "INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 5)\n",
      "INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 786, 2)\n",
      "INFO:root:No optional prior non-batchable context provided.\n",
      "INFO:root:No optional prior batchable context provided.\n",
      "INFO:root:Shape of simulation non-batchable context: ()\n",
      "INFO:root:Could not determine shape of simulation batchable context. Type appears to be non-array: <class 'list'>,                                    so make sure your input configurator takes cares of that!\n",
      "INFO:root:Loaded loss history from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\t_df1\\history_100.pkl.\n",
      "INFO:root:Networks loaded from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\t_df1\\ckpt-100\n",
      "INFO:root:Performing a consistency check with provided components...\n",
      "INFO:root:Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amortized_posterior_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ddm_inference (InvertibleN  multiple                  434037    \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " set_transformer_1 (SetTran  multiple                  46544     \n",
      " sformer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 480581 (1.83 MB)\n",
      "Trainable params: 480521 (1.83 MB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "@nb.jit(nopython=True)\n",
    "\n",
    "def diffusion_experiment_contaminated_t_df1(theta, design_matrix, num_obs, rng=None, *args):\n",
    "    out = np.zeros((num_obs, 2))\n",
    "    for n in range(num_obs):\n",
    "        index = design_matrix[n]\n",
    "        rt,resp = diffusion_trial(theta[index], theta[-3], theta[-2], theta[-1])\n",
    "        out[n, :] = np.array([rt,resp])\n",
    "    \n",
    "    #log transformation\n",
    "    out[:,0] = np.log(out[:,0])\n",
    "    \n",
    "    CC=np.random.standard_t(df=1,size=num_obs)\n",
    "    X=np.random.binomial(n=1,p=.1,size=num_obs)\n",
    "    out[:,0] = (1-X)*out[:,0] + (X)*np.log(np.abs(CC))\n",
    "    out[:,1] = (1-X)*out[:,1] + (X)*np.random.binomial(n=1,p=0.5,size=num_obs)\n",
    "    \n",
    "    return out\n",
    "\n",
    "simulator_t_df1 = bf.simulation.Simulator(simulator_fun=diffusion_experiment_contaminated_t_df1, context_generator=context_gen)\n",
    "\n",
    "model_t_df1 = bf.simulation.GenerativeModel(prior=prior, simulator=simulator_t_df1)\n",
    "\n",
    "#SetTransformer() is a permutation invariant network\n",
    "#input_dim = how many dimensions the configured data would have\n",
    "#here we have a RT, dummy coded variable for the two choices (one or two?), the context dummy\n",
    "summary_net_t_df1 = bf.networks.SetTransformer(input_dim=4, summary_dim=12)\n",
    "\n",
    "#we are turning off the kernel and dropout regularization for the networks \n",
    "#since we don’t need these for online training\n",
    "inference_net_t_df1 = bf.networks.InvertibleNetwork(\n",
    "    num_params=len(prior.param_names),\n",
    "    coupling_design=\"interleaved\",\n",
    "    name=\"ddm_inference\",\n",
    ")\n",
    "\n",
    "amortizer_t_df1 = bf.amortizers.AmortizedPosterior(inference_net_t_df1, summary_net_t_df1)\n",
    "\n",
    "\n",
    "trainer_t_df1 = bf.trainers.Trainer(\n",
    "    generative_model=model_t_df1, amortizer=amortizer_t_df1, configurator=diffusion_configurator,\n",
    "    checkpoint_path = checkpoint_path_t_df1\n",
    ")\n",
    "amortizer_t_df1.summary()\n",
    "\n",
    "#history = trainer_c_t_df1.train_online(epochs=100, iterations_per_epoch=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a053fd",
   "metadata": {},
   "source": [
    "### Robust estimator (t, df=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "451a7864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Performing 2 pilot runs with the anonymous model...\n",
      "INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 5)\n",
      "INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 626, 2)\n",
      "INFO:root:No optional prior non-batchable context provided.\n",
      "INFO:root:No optional prior batchable context provided.\n",
      "INFO:root:Shape of simulation non-batchable context: ()\n",
      "INFO:root:Could not determine shape of simulation batchable context. Type appears to be non-array: <class 'list'>,                                    so make sure your input configurator takes cares of that!\n",
      "INFO:root:Loaded loss history from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\t_df3\\history_100.pkl.\n",
      "INFO:root:Networks loaded from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\t_df3\\ckpt-100\n",
      "INFO:root:Performing a consistency check with provided components...\n",
      "INFO:root:Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amortized_posterior_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ddm_inference (InvertibleN  multiple                  434037    \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " set_transformer_2 (SetTran  multiple                  46544     \n",
      " sformer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 480581 (1.83 MB)\n",
      "Trainable params: 480521 (1.83 MB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "@nb.jit(nopython=True)\n",
    "\n",
    "def diffusion_experiment_contaminated_t_df3(theta, design_matrix, num_obs, rng=None, *args):\n",
    "    out = np.zeros((num_obs, 2))\n",
    "    for n in range(num_obs):\n",
    "        index = design_matrix[n]\n",
    "        rt,resp = diffusion_trial(theta[index], theta[-3], theta[-2], theta[-1])\n",
    "        out[n, :] = np.array([rt,resp])\n",
    "    \n",
    "    #log transformation\n",
    "    out[:,0] = np.log(out[:,0])\n",
    "    \n",
    "    CC=np.random.standard_t(df=3,size=num_obs)\n",
    "    X=np.random.binomial(n=1,p=.1,size=num_obs)\n",
    "    out[:,0] = (1-X)*out[:,0] + (X)*np.log(np.abs(CC))\n",
    "    out[:,1] = (1-X)*out[:,1] + (X)*np.random.binomial(n=1,p=0.5,size=num_obs)\n",
    "    \n",
    "    return out\n",
    "\n",
    "simulator_t_df3 = bf.simulation.Simulator(simulator_fun=diffusion_experiment_contaminated_t_df3, context_generator=context_gen)\n",
    "\n",
    "model_t_df3 = bf.simulation.GenerativeModel(prior=prior, simulator=simulator_t_df3)\n",
    "\n",
    "#SetTransformer() is a permutation invariant network\n",
    "#input_dim = how many dimensions the configured data would have\n",
    "#here we have a RT, dummy coded variable for the two choices (one or two?), the context dummy\n",
    "summary_net_t_df3 = bf.networks.SetTransformer(input_dim=4, summary_dim=12)\n",
    "\n",
    "#we are turning off the kernel and dropout regularization for the networks \n",
    "#since we don’t need these for online training\n",
    "inference_net_t_df3 = bf.networks.InvertibleNetwork(\n",
    "    num_params=len(prior.param_names),\n",
    "    coupling_design=\"interleaved\",\n",
    "    name=\"ddm_inference\",\n",
    ")\n",
    "\n",
    "amortizer_t_df3 = bf.amortizers.AmortizedPosterior(inference_net_t_df3, summary_net_t_df3)\n",
    "\n",
    "\n",
    "trainer_t_df3 = bf.trainers.Trainer(\n",
    "    generative_model=model_t_df3, amortizer=amortizer_t_df3, configurator=diffusion_configurator,\n",
    "    checkpoint_path = checkpoint_path_t_df3\n",
    ")\n",
    "amortizer_t_df3.summary()\n",
    "\n",
    "#history = trainer_c_t_df3.train_online(epochs=100, iterations_per_epoch=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757a623",
   "metadata": {},
   "source": [
    "### Robust estimator (t, df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0a43e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Performing 2 pilot runs with the anonymous model...\n",
      "INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 5)\n",
      "INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 907, 2)\n",
      "INFO:root:No optional prior non-batchable context provided.\n",
      "INFO:root:No optional prior batchable context provided.\n",
      "INFO:root:Shape of simulation non-batchable context: ()\n",
      "INFO:root:Could not determine shape of simulation batchable context. Type appears to be non-array: <class 'list'>,                                    so make sure your input configurator takes cares of that!\n",
      "INFO:root:Loaded loss history from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\t_df5\\history_100.pkl.\n",
      "INFO:root:Networks loaded from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\t_df5\\ckpt-100\n",
      "INFO:root:Performing a consistency check with provided components...\n",
      "INFO:root:Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amortized_posterior_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ddm_inference (InvertibleN  multiple                  434037    \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " set_transformer_3 (SetTran  multiple                  46544     \n",
      " sformer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 480581 (1.83 MB)\n",
      "Trainable params: 480521 (1.83 MB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "@nb.jit(nopython=True)\n",
    "\n",
    "def diffusion_experiment_contaminated_t_df5(theta, design_matrix, num_obs, rng=None, *args):\n",
    "    out = np.zeros((num_obs, 2))\n",
    "    for n in range(num_obs):\n",
    "        index = design_matrix[n]\n",
    "        rt,resp = diffusion_trial(theta[index], theta[-3], theta[-2], theta[-1])\n",
    "        out[n, :] = np.array([rt,resp])\n",
    "    \n",
    "    #log transformation\n",
    "    out[:,0] = np.log(out[:,0])\n",
    "    \n",
    "    CC=np.random.standard_t(df=5,size=num_obs)\n",
    "    X=np.random.binomial(n=1,p=.1,size=num_obs)\n",
    "    out[:,0] = (1-X)*out[:,0] + (X)*np.log(np.abs(CC))\n",
    "    out[:,1] = (1-X)*out[:,1] + (X)*np.random.binomial(n=1,p=0.5,size=num_obs)\n",
    "    \n",
    "    return out\n",
    "\n",
    "simulator_t_df5 = bf.simulation.Simulator(simulator_fun=diffusion_experiment_contaminated_t_df5, context_generator=context_gen)\n",
    "\n",
    "model_t_df5 = bf.simulation.GenerativeModel(prior=prior, simulator=simulator_t_df5)\n",
    "\n",
    "#SetTransformer() is a permutation invariant network\n",
    "#input_dim = how many dimensions the configured data would have\n",
    "#here we have a RT, dummy coded variable for the two choices (one or two?), the context dummy\n",
    "summary_net_t_df5 = bf.networks.SetTransformer(input_dim=4, summary_dim=12)\n",
    "\n",
    "#we are turning off the kernel and dropout regularization for the networks \n",
    "#since we don’t need these for online training\n",
    "inference_net_t_df5 = bf.networks.InvertibleNetwork(\n",
    "    num_params=len(prior.param_names),\n",
    "    coupling_design=\"interleaved\",\n",
    "    name=\"ddm_inference\",\n",
    ")\n",
    "\n",
    "amortizer_t_df5 = bf.amortizers.AmortizedPosterior(inference_net_t_df5, summary_net_t_df5)\n",
    "\n",
    "\n",
    "trainer_t_df5 = bf.trainers.Trainer(\n",
    "    generative_model=model_t_df5, amortizer=amortizer_t_df5, configurator=diffusion_configurator,\n",
    "    checkpoint_path = checkpoint_path_t_df5\n",
    ")\n",
    "amortizer_t_df5.summary()\n",
    "\n",
    "#history = trainer_c_t_df5.train_online(epochs=100, iterations_per_epoch=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc53d8b",
   "metadata": {},
   "source": [
    "### Robust estimator (U(0,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e0d7671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Performing 2 pilot runs with the anonymous model...\n",
      "INFO:root:Shape of parameter batch after 2 pilot simulations: (batch_size = 2, 5)\n",
      "INFO:root:Shape of simulation batch after 2 pilot simulations: (batch_size = 2, 278, 2)\n",
      "INFO:root:No optional prior non-batchable context provided.\n",
      "INFO:root:No optional prior batchable context provided.\n",
      "INFO:root:Shape of simulation non-batchable context: ()\n",
      "INFO:root:Could not determine shape of simulation batchable context. Type appears to be non-array: <class 'list'>,                                    so make sure your input configurator takes cares of that!\n",
      "INFO:root:Loaded loss history from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\uniform\\history_100.pkl.\n",
      "INFO:root:Networks loaded from C:\\Users\\u0145642\\OneDrive - KU Leuven\\Desktop\\Robust amortized Bayesian inference\\3 ddm\\networks\\uniform\\ckpt-100\n",
      "INFO:root:Performing a consistency check with provided components...\n",
      "INFO:root:Done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"amortized_posterior_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ddm_inference (InvertibleN  multiple                  434037    \n",
      " etwork)                                                         \n",
      "                                                                 \n",
      " set_transformer_5 (SetTran  multiple                  46544     \n",
      " sformer)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 480581 (1.83 MB)\n",
      "Trainable params: 480521 (1.83 MB)\n",
      "Non-trainable params: 60 (240.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "@nb.jit(nopython=True,parallel=True)\n",
    "def diffusion_experiment_contaminated_uniform(theta, design_matrix, num_obs, rng=None, *args):\n",
    "    \n",
    "    out = np.zeros((num_obs, 2))\n",
    "    for n in range(num_obs):\n",
    "        index = design_matrix[n]\n",
    "        rt,resp = diffusion_trial(theta[index], theta[-3], theta[-2], theta[-1])\n",
    "        out[n, :] = np.array([rt,resp])\n",
    "    \n",
    "    out[:,0] = np.log(out[:,0])\n",
    "    \n",
    "    CC=np.random.uniform(0,20,size=num_obs)\n",
    "    X=np.random.binomial(n=1,p=.1,size=num_obs)\n",
    "    out[:,0] = (1-X)*out[:,0] + (X)*np.log(CC)\n",
    "    out[:,1] = (1-X)*out[:,1] + (X)*np.random.binomial(n=1,p=0.5,size=num_obs)\n",
    "    \n",
    "    return out\n",
    "\n",
    "simulator_uniform = bf.simulation.Simulator(simulator_fun=diffusion_experiment_contaminated_uniform, context_generator=context_gen)\n",
    "\n",
    "model_uniform = bf.simulation.GenerativeModel(prior=prior, simulator=simulator_uniform)\n",
    "\n",
    "summary_net_uniform = bf.networks.SetTransformer(input_dim=4, summary_dim=12)\n",
    "\n",
    "#we are turning off the kernel and dropout regularization for the networks \n",
    "#since we don’t need these for online training\n",
    "inference_net_uniform = bf.networks.InvertibleNetwork(\n",
    "    num_params=len(prior.param_names),\n",
    "    coupling_design=\"interleaved\",\n",
    "    name=\"ddm_inference\",\n",
    ")\n",
    "\n",
    "amortizer_uniform = bf.amortizers.AmortizedPosterior(inference_net_uniform, summary_net_uniform)\n",
    "\n",
    "\n",
    "trainer_uniform = bf.trainers.Trainer(\n",
    "    generative_model=model_uniform, \n",
    "    amortizer=amortizer_uniform, \n",
    "    configurator=diffusion_configurator,\n",
    "    checkpoint_path = checkpoint_path_uniform\n",
    ")\n",
    "amortizer_uniform.summary()\n",
    "\n",
    "#history = trainer_c_uniform.train_online(epochs=100, iterations_per_epoch=1000, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
